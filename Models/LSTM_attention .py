# -*- coding: utf-8 -*-
"""LSTM-attention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yEwEwHDlkRcLmkc0G9tGAAROIZZLkdf0
"""

import torch.nn as nn
import torch
from attention import Attention
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
def attention_lstm(inputs):
    # inputs.shape = (batch_size, time_steps, input_dim)
    print(inputs.shape)
    input_dim = int(inputs.shape[2])
    a = Permute((2, 1))(inputs)
    time_steps = 3
    a = Reshape((input_dim, time_steps))(a)
    a = Dense(time_steps, activation='softmax')(a)
    if single:
        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)
        a = RepeatVector(input_dim)(a)
    a_probs = Permute((2, 1), name='attention_vec')(a)
    attention_m = multiply([inputs, a_probs])
    return attention_m



class lstm_a(nn.Module) :
    def __init__(self, seq_len=256, ini_len=18, final_len=1) :
        super().__init__()
        self.d_model = ini_len 
        self.seq_len = seq_len
        self.hidden_size = 32
        self.num_layers = 1
        self.lstm = nn.LSTM(self.d_model,self.hidden_size,self.num_layers,batch_first=True)
        attention_m = attention_lstm(self.lstm)
        attention_m = Flatten()(attention_m) 
        output = Dense(units=1) (attention_m)
        # self.final = nn.Sequential(nn.Linear(self.hidden_size*self.seq_len,512),nn.ReLU(),nn.Linear(512,final_len))
    
    def forward(self,batch) :
        batch, (_b,_a) = self.lstm_a( batch )
        del _b, _a
        out = self.final(batch.reshape(-1,self.hidden_size*self.seq_len))
        return out
